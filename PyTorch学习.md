## 第一讲

PyTorch入门
如何实现一个系统
理解卷积神经网络
线性代数和概率论
python

深度学习
多层感知机
卷积神经网络
循环神经网络
各种经典网络
LeNet、AlexNet、GoogLeNet、VGG、 ResNets、Inception、GAN

![2020-11-09_065530](C:\Users\高清南\Desktop\2020-11-09_065530.jpg)

深度学习网络
张量和计算图
张量可以使得矩阵在GPU工作
计算图可以方便使用反向传播，链式求导，实现自动微分

## 第二讲

dataset
model
training
infer

kaggle竞赛
给出训练集，和训练集x的值，上传代码

绘图，判断训练程度
绘图工具visdom

## 第三讲

梯度下降
深度神经网络很难陷入局部最优
解决鞍点问题
随机梯度下降SGD和mini-batch
随机梯度下降并行性低，可以找到最优点

## 第四讲

y=W2(W1*X+b1)+b2
=W*X+b
不加激活函数，一直是线性的

张量保存权重和损失关于权重的导数
Tensor做运算会形成计算图
loss()  构建计算图
backward 反向传播，自动微分

## 第五讲

前馈、反馈、更新
训练过程中，绘制训练集误差和测试集误差曲线，防止过拟合

##  第六讲

逻辑回归损失函数
概率角度，极大似然估计MLE
交叉熵
梯度下降就是使得真实标签和预测标签接近
框架：规范、API
torchvision中包含数据集

## 第七讲

读文档

## 第八讲

mini-batch
epoch：整个数据集过一遍
iteration：一次梯度下降
kaggle数据集
python的魔法函数

## 第九讲

数据集、训练模型
构造损失函数和优化器
执行前馈反馈更新
快速傅里叶变换、小波变换

## 第十讲

卷积神经网络CNN
Convolution Neutral Network
输入层：channel* width*height
卷积层由四个参数确定：
输入通道数，输出通道数、卷积核大小
输入通道数决定了卷积核的通道数，两者一一对应
卷积核的个数决定了输出通道数，两者一一对应
输入层的宽高和卷积核的大小决定了输出层的宽高
卷积层增加通道数
池化层减少宽高
卷积层的卷积核是需要学习的参数
池化层是一个固定的映射，不需要学习，只是为了减少尺寸和计算量，防止过拟合
卷积层的两个特征：
局部连接、权值共享
self.conv1 = torch.nn.Conv2d(1,10,kernel_size=5)
真正网络训练时，只需要指定输入通道数、输出通道数、卷积核尺寸即可
然后输入数据即原始宽高，可以确定卷积核的通道数，卷积核的个数，输出的尺寸
卷积核的参数在训练过程中求得

## 第十讲

多个输出的网络
Inception  网络自动决定超参数
b c w h 四个维度

Resnet 基于更深的网络不应该更差，随着深度增加，网络性能变差，Resnet使用残差块
解决梯度消失问题
Relu函数
逐层训练，微调，找到局部最优，再找全局最优
残差网络Resnet

## 第十一讲

RNN
循环神经网络存在的问题
预测时，只用到了前面的信息，可以使用双向循环神经网络
RNN利用了之前的信息，并且存在长期依赖问题
解决gradient vanish 和gradient explode
使用LSTM和GRU

循环神经网络实现了同层连接，残差网络实现了跨层连接
多层感知机实现的是邻层连接

循环神经网络的输入输出是不固定的
循环神经网络参数在不同的时刻是共享的
循环神经网络是上下文相关的
循环神经网络在层内使用tanh函数，输出使用sigmoid函数

循环神经网络通过for循环实现
h1=Lindear（x1，h0）
h2=Linear（x2，h1）
每一个时刻共用一套权重
x和h经过不同的线性激活，可以组合起来，看作经过一个线性层
将x1和h0的节点组合成上一层，下一层是h1，然后循环
cell=torch.nn.RNNCell(input_size,hidden_size)
torch.nn.RNN(input_size,hidden_size,num_layers
不管时间维度如何展开，一个RNN对应一个线性层
四个参数：
batch_size 每次处理的元素的个数，默认一个
sql_len 每一个序列元素的个数，可以理解成一个序列有多少时刻
input_size 每一个时刻或者元素，对应的特征数
hidden_size 每一个时刻对应的输出特征数
一个序列对应整个for循环，完成一个序列的处理
一次for循环对应一个元素 ，输入当前时刻（元素）x1和上一个隐层h0
整个for循环共用一个权重矩阵，只有一个线性层
通过每一个时刻将网络对齐，通过控制for循环次数，对应不同的序列长度

使用Embed层将独热编码变成稠密表示
###### LSTM提供了梯度传播的一条更直接的路径，防止梯度消失
不用管各种门，只需要看公式

## 第十二讲

GRU和LSTM都是一种特殊的RNN
RNN训练的每一个序列相当于普通网络的一个样本
序列中的每一个时刻共用一个权重矩阵，实现了参数共享
标准RNN将每一个序列中的上一个时刻输出和当前时刻合并成一层，作为当前时刻输入，
通过全连接层，h1=w(x1+h0)+b，可以直接将h1作为当前输出，y1=h1
可以再通过一个全连接层，h2=wh1+b
应用案例：
多个名字和对应的国家
每个名字对应一个序列，一个序列由若干字符组成
每个字符对应一个时刻可以用一个one-hot向量表示
通过基于时间的反向传播，调整参数
建立一个输入到输出的非线性函数
输入一个待预测序列，可以预测出国家









